{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Custom Models and Training with Tensorflow\n",
    "\n",
    "\n",
    "\n",
    "Creating a new loss function allows you to store the config, load from a config and apply ('call') the method.\n",
    "\n",
    "Initializers, Regularizers, Constraings can be overwriten. A kernel_constraint allows you to overwrite the edge\n",
    "weights \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version  2.3.0\n",
      "Keras version  2.4.0\n"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.image import imread\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TF version \", tf.__version__)\n",
    "print(\"Keras version \", keras.__version__)\n",
    "\n",
    "# Custom error handler for the entire notebook so stack traces are not lost\n",
    "from IPython.core.ultratb import AutoFormattedTB\n",
    "\n",
    "# initialize the formatter for making the tracebacks into strings\n",
    "itb = AutoFormattedTB(mode = 'Plain', tb_offset = 1)\n",
    "\n",
    "# Define a global with the stack trace that we can append to in the handler.\n",
    "viki_stack_trace = ''\n",
    "\n",
    "# this function will be called on exceptions in any cell\n",
    "def custom_exc(shell, etype, evalue, tb, tb_offset=None):\n",
    "    global viki_stack_trace\n",
    "\n",
    "    # still show the error within the notebook, don't just swallow it\n",
    "    shell.showtraceback((etype, evalue, tb), tb_offset=tb_offset)\n",
    "\n",
    "    # grab the traceback and make it into a list of strings\n",
    "    stb = itb.structured_traceback(etype, evalue, tb)\n",
    "    sstb = itb.stb2text(stb)\n",
    "\n",
    "    print (sstb) # <--- this is the variable with the traceback string\n",
    "    viki_stack_trace = viki_stack_trace + sstb\n",
    "\n",
    "# this registers a custom exception handler for the whole current notebook\n",
    "get_ipython().set_custom_exc((Exception,), custom_exc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Loss\" functions are used during training, and their gradient is what is optimized.\n",
    "\n",
    "By contrast, \"metrics\" are used to evaluate a model, they can be anything arbitrary. They have no expectation of\n",
    "having nonzero values or existence of gradients.\n",
    "\n",
    "This is a custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    \"A custom loss function that will be used later. Just an example\"\n",
    "\n",
    "    def __init(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"Evaluate the loss at this stage\"\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold ** 2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"Called when model is saved to preserve existing config. This class will save its parent class' config too.\"\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are other custom functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_softplus(z):\n",
    "    \"Used to return a probability of seeing this output\"\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def initializer_glorot(shape, dtype=tf.float32):\n",
    "    \"Used to initialize weights before training\"\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def regularizer_l1(weights):\n",
    "    \"Used to avoid over-fitting, and keep weights meaningful\"\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def constraint_weights(weights):\n",
    "    \"Applied after the training to constrain the weights at the layer arbitrarily\"\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above methods can be used directly, but we can also create a class that inherits from\n",
    "keras.initializers.Initializer, keras.regularizers.Regularizer, and keras.constraints.Constraint appropriately.\n",
    "The activation function usually has nothing to save, so if you want to have a parameter for the activation, you can create a new layer type.\n",
    "\n",
    "Here's an example of extending just one of them, the Regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VikiL1(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        \"Create a regularizer with L1 regularization and the factor provided here\"\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        \"Apply this regularizer with the weights at this layer\"\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"Returns the configuration of this class for application later\"\n",
    "        return {\"factor\": self.factor} # We don't look up the parent's config, because it has none.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom layer can be implemented that does add_weight() for all the values it needs to keep track of, and in the call() method, it provides the output from this layer. I don't quite understand how gradients are calculated at every layer. Perhaps the exercises make this clearer.\n",
    "\n",
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tensorflow is a library for training, saving/restoring and applying models that runs fast on GPUs, can scale automatically to CPUs as available. It is built using Numpy arrays, and provides functionality like automatic differentiation of code, fast numerical routines, and a vibrant ecosystem of models and implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tensorflow is not a drop-in replacement to Numpy. Tensorflow produces tensorflow operations (ops) rather than native execution blocks. The computation graph is created by Tensorflow and then is executed by it. This is different from numpy which produces either native code or python code.\n",
    "\n",
    "TF is also meant to be used as a way to train NN models, store them, and run them elsewhere. This could be done on better machines like GPUs, or worse machines like mobile phones with tflite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. tf.range(10) gives a single tensor of shape \\(10\\), while tf.constant\\(np.arange\\(10)) should give ten constants? Let's try this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(np.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're identical except for the Numpy version started out with 64-bit ints, and those were used by TF too. TF by itself defautls to int32 (to make things faster on GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Six other data structures:\n",
    "   1. Tensor arrays. Lists of tensors.\n",
    "   2. String Tensors\n",
    "   3. Ragged tensors (tf.RaggedTensor)\n",
    "   4. Sparse matrices?\n",
    "   5. Sets\n",
    "   6. Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. I'd use a function if I don't have parameters or I don't want the custom loss function saved along with the model. Otherwise I'd use a subclass of keras.losses.Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Similar to the previous option, I'd use a custom metric class if I intend to report on the metric during model training, and thus would like to store it, or to parameterize it somehow and remember the parameters. I would also use a custom metric in case it was a streaming metric, and it needed to remember something past this invocation to have the correct metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
