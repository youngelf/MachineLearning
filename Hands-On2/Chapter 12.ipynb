{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Custom Models and Training with Tensorflow\n",
    "\n",
    "\n",
    "\n",
    "Creating a new loss function allows you to store the config, load from a config and apply ('call') the method.\n",
    "\n",
    "Initializers, Regularizers, Constraings can be overwriten. A kernel_constraint allows you to overwrite the edge weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.image import imread\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TF version \", tf.__version__)\n",
    "print(\"Keras version \", keras.__version__)\n",
    "\n",
    "# Custom error handler for the entire notebook so stack traces are not lost\n",
    "from IPython.core.ultratb import AutoFormattedTB\n",
    "\n",
    "# initialize the formatter for making the tracebacks into strings\n",
    "itb = AutoFormattedTB(mode = 'Plain', tb_offset = 1)\n",
    "\n",
    "# Define a global with the stack trace that we can append to in the handler.\n",
    "viki_stack_trace = ''\n",
    "\n",
    "# this function will be called on exceptions in any cell\n",
    "def custom_exc(shell, etype, evalue, tb, tb_offset=None):\n",
    "    global viki_stack_trace\n",
    "\n",
    "    # still show the error within the notebook, don't just swallow it\n",
    "    shell.showtraceback((etype, evalue, tb), tb_offset=tb_offset)\n",
    "\n",
    "    # grab the traceback and make it into a list of strings\n",
    "    stb = itb.structured_traceback(etype, evalue, tb)\n",
    "    sstb = itb.stb2text(stb)\n",
    "\n",
    "    print (sstb) # <--- this is the variable with the traceback string\n",
    "    viki_stack_trace = viki_stack_trace + sstb\n",
    "\n",
    "# this registers a custom exception handler for the whole current notebook\n",
    "get_ipython().set_custom_exc((Exception,), custom_exc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Loss\" functions are used during training, and their gradient is what is optimized.\n",
    "\n",
    "By contrast, \"metrics\" are used to evaluate a model, they can be anything arbitrary. They have no expectation of\n",
    "having nonzero values or existence of gradients.\n",
    "\n",
    "This is a custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    \"A custom loss function that will be used later. Just an example\"\n",
    "\n",
    "    def __init(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"Evaluate the loss at this stage\"\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold ** 2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"Called when model is saved to preserve existing config. This class will save its parent class' config too.\"\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are other custom functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_softplus(z):\n",
    "    \"Used to return a probability of seeing this output\"\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def initializer_glorot(shape, dtype=tf.float32):\n",
    "    \"Used to initialize weights before training\"\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def regularizer_l1(weights):\n",
    "    \"Used to avoid over-fitting, and keep weights meaningful\"\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def constraint_weights(weights):\n",
    "    \"Applied after the training to constrain the weights at the layer arbitrarily\"\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above methods can be used directly, but we can also create a class that inherits from\n",
    "keras.initializers.Initializer, keras.regularizers.Regularizer, and keras.constraints.Constraint appropriately.\n",
    "The activation function usually has nothing to save, so if you want to have a parameter for the activation, you can create a new layer type.\n",
    "\n",
    "Here's an example of extending just one of them, the Regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VikiL1(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        \"Create a regularizer with L1 regularization and the factor provided here\"\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        \"Apply this regularizer with the weights at this layer\"\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"Returns the configuration of this class for application later\"\n",
    "        return {\"factor\": self.factor} # We don't look up the parent's config, because it has none.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom layer can be implemented that does add_weight() for all the values it needs to keep track of, and in the call() method, it provides the output from this layer. I don't quite understand how gradients are calculated at every layer. Perhaps the exercises make this clearer.\n",
    "\n",
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tensorflow is a library for training, saving/restoring and applying models that runs fast on GPUs, can scale automatically to CPUs as available. It is built using Numpy arrays, and provides functionality like automatic differentiation of code, fast numerical routines, and a vibrant ecosystem of models and implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tensorflow is not a drop-in replacement to Numpy. Tensorflow produces tensorflow operations (ops) rather than native execution blocks. The computation graph is created by Tensorflow and then is executed by it. This is different from numpy which produces either native code or python code.\n",
    "\n",
    "TF is also meant to be used as a way to train NN models, store them, and run them elsewhere. This could be done on better machines like GPUs, or worse machines like mobile phones with tflite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. tf.range(10) gives a single tensor of shape \\(10\\), while tf.constant\\(np.arange\\(10)) should give ten constants? Let's try this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant(np.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're identical except for the Numpy version started out with 64-bit ints, and those were used by TF too. TF by itself defaults to int32 (to make things faster on GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Six other data structures:\n",
    "   1. Tensor arrays. Lists of tensors.\n",
    "   2. String Tensors\n",
    "   3. Ragged tensors (tf.RaggedTensor)\n",
    "   4. Sparse matrices?\n",
    "   5. Sets\n",
    "   6. Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. I'd use a function if I don't have parameters or I don't want the custom loss function saved along with the model. Otherwise I'd use a subclass of keras.losses.Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Similar to the previous option, I'd use a custom metric class if I intend to report on the metric during model training, and thus would like to store it, or to parameterize it somehow and remember the parameters. I would also use a custom metric in case it was a streaming metric, and it needed to remember something past this invocation to have the correct metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. I'd make a custom layer when I plan to use the layer frequently in other models, while a full model when the structure of the full model itself can be reused frequently for other kinds of data without further modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. You'd do your own training loop if you want gradients propagated differently. The book says that in some implementations, you want different optimizers for different sections of the graph. That's another reason.\n",
    "\n",
    "\n",
    "9. Keras components should be convertible to TF. If they have Python code, they will be evaluated once, and the values will be used on subsequent runs. For example, if a random value is needed on every training run of the loop, it should be a TF operation. If instead it is a Python code, that value will be calculated once and its value used thereafter.\n",
    "\n",
    "10. Call primitive operations, as far as possible. Don't call compiled code. Change sum() to tf.reduce_sum(). Be mindful about side-effects because they might not be caused on every run.\n",
    "\n",
    "11. The book doesn't use the word 'dynamic' Keras model, but I think this is the same as the Residual example given. You use a dynamic model when you want to do things the sequential API doens't make easy: skip layers, two parallel paths that join up and then split again, or skipping layers later in the training, or based on the residual remaining.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Create a custom layer that does layer normalization. Ooh-Kaay. This is going to be fun.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VikiNormalizedLayer(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        # The general initialization routine, parse the normal args and remember the units.\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Perform layer normalization here\n",
    "        mean, variance = tf.nn.moments(inputs, axes=-1, keepdims=True)\n",
    "        std_dev = tf.math.sqrt(variance)\n",
    "        # Eps is a small smoothing factor, selected to be everyones favorite: 0.001 here.\n",
    "        eps = 0.001\n",
    "        return self.alpha @ (inputs - mean) / (std_dev + eps) + self.beta\n",
    "    \n",
    "    def build(self, batch_input_shape):\n",
    "        # Define two trainable weights: alpha and beta, which are the same shape as the previous out\n",
    "        # and float32.\n",
    "        self.alpha = self.add_weight(name=\"alpha\", shape=[batch_input_shape[-1], self.units],\n",
    "                                    initializer=\"ones\")\n",
    "        self.beta = self.add_weight(name=\"beta\", shape=[batch_input_shape[-1], self.units],\n",
    "                                    initializer=\"zeros\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was too silent, I bet this is not working. How do we test out that layer? Let's create an NN using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:  (10000, 32, 32, 3)\n",
      "Training:  (40000, 32, 32, 3)\n",
      "Labels validation:  (10000,)\n",
      "Labels training:  (40000,)\n",
      "Test:  (10000, 32, 32, 3)\n",
      "Labels test:  (10000, 1)\n",
      "Model built:  <tensorflow.python.keras.engine.sequential.Sequential object at 0x7faf8e230e20>\n"
     ]
    }
   ],
   "source": [
    "# Let's load the data\n",
    "\n",
    "(X, y), (testX, testy) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Split into training and testing\n",
    "X_train, X_valid = X[:40000] / 255.0, X[40000:] / 255.0\n",
    "y = y.reshape(50000)\n",
    "testy.reshape(10000)\n",
    "\n",
    "y_train, y_valid = y[:40000]        , y[40000:]\n",
    "\n",
    "print(\"Validation: \", X_valid.shape)\n",
    "print(\"Training: \", X_train.shape)\n",
    "print(\"Labels validation: \", y_valid.shape)\n",
    "print(\"Labels training: \", y_train.shape)\n",
    "\n",
    "print(\"Test: \", testX.shape)\n",
    "print(\"Labels test: \", testy.shape)\n",
    "\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "def create_keras_classifier_model(n_classes=100):\n",
    "    \"\"\"Keras multinomial logistic regression creation model\n",
    " \n",
    "    Args:\n",
    "        n_classes(int): Number of classes to be classified\n",
    " \n",
    "    Returns:\n",
    "        Compiled keras model\n",
    " \n",
    "    \"\"\"\n",
    "    # create model\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # The input: we get 32x32 pixels, each with 3 colors (rgb)\n",
    "    model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "    # Then the hidden layers, fully connected (100 by default)\n",
    "    for i in range(20):\n",
    "        model.add(keras.layers.Dense(\n",
    "            n_classes, \n",
    "            activation=\"elu\",\n",
    "            kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "        ))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "    # Now add the output layer: 10 classes in CIFAR10, so 10 outputs.\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    # print(model.summary())\n",
    "    # Compile model\n",
    "    nadam = keras.optimizers.Nadam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", \n",
    "        optimizer=nadam,\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Clear the errors, in case we observe them in the long run.\n",
    "viki_stack_trace = ''\n",
    "\n",
    "# Got to remember them. mm_bn is the model with Batch normalization\n",
    "mm_bn = create_keras_classifier_model(100)\n",
    "print (\"Model built: \", mm_bn)\n",
    "\n",
    "history_bn = mm_bn.fit(X_train, y_train, epochs=10, verbose=0,\n",
    "                 batch_size=32,\n",
    "                 validation_data=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on that, let's create a model using my Normalization layer\n",
    "\n",
    "def create_keras_classifier_model(n_classes=100):\n",
    "    \"\"\"Keras multinomial logistic regression creation model\n",
    " \n",
    "    Args:\n",
    "        n_classes(int): Number of classes to be classified\n",
    " \n",
    "    Returns:\n",
    "        Compiled keras model\n",
    " \n",
    "    \"\"\"\n",
    "    # create model\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # The input: we get 32x32 pixels, each with 3 colors (rgb)\n",
    "    model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "    # Then the hidden layers, fully connected (100 by default)\n",
    "    for i in range(3):\n",
    "        model.add(keras.layers.Dense(\n",
    "            n_classes, \n",
    "            activation=\"elu\",\n",
    "            kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "        ))\n",
    "        model.add(VikiNormalizationLayer())\n",
    "    # Now add the output layer: 10 classes in CIFAR10, so 10 outputs.\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    # print(model.summary())\n",
    "    # Compile model\n",
    "    nadam = keras.optimizers.Nadam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", \n",
    "        optimizer=nadam,\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Clear the errors, in case we observe them in the long run.\n",
    "viki_stack_trace = ''\n",
    "\n",
    "# Got to remember them. mm_bn is the model with Batch normalization\n",
    "mm_bn = create_keras_classifier_model(100)\n",
    "print (\"Model built: \", mm_bn)\n",
    "\n",
    "history_bn = mm_bn.fit(X_train, y_train, epochs=10, verbose=0,\n",
    "                 batch_size=32,\n",
    "                 validation_data=(X_valid, y_valid))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
