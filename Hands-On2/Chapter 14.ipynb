{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14: Convolutional Networks, Deep vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
    "dataset_size = info.splits[\"train\"].num_examples\n",
    "\n",
    "class_names = info.features[\"label\"].names\n",
    "n_classes = info.features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this in normal Python instead because even loading the dataset fails when there is no Javascript suppport.\n",
    "\n",
    "Seriously, this is really brittle stuff, and it is surprising that this is the backbone of the hottest trend of 2020.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. A CNN has fewer parameters than an equivalent DNN. It also takes advantage of spatial information better than a DNN. Finally, a CNN can be made somewhat independent of input image size using Fully Convolutional Networks (FCNs)\n",
    "\n",
    "2. \n",
    "\n",
    "3. If a GPU runs out of memory, techniques:\n",
    " * Reduce batch size\n",
    " * Reduce the number of parameters (increasing stride, increasing filter size, reducing layers)\n",
    " * Get a better GPU\n",
    " * Get more GPU instances to split the training load\n",
    " * Run training on a CPU instead, at slower speed.\n",
    "\n",
    "4. A max-pooling layer can aggregate information from various layers, and also be more resilient to variations in any one layer. A CNN, by contrast, will consider all the values and might be too sensitive to the individual values.\n",
    "\n",
    "5. A local response normalization helps to get the network to specialize for different features, rather than having the model pick up on the same features in different nodes.\n",
    "\n",
    "6. Different features\n",
    " * LeNet: Introduced the idea of Convolution and Pooling\n",
    " * AlexNet: Had convolution layers on top of convolution layers\n",
    " * GoogleNet: Had Inception modules, with skip connections, and different scales to learn features at different levels of granularity.\n",
    " * ResNet: Added residual connections to avoid vanishing gradients. Allowed the network to train faster, and be resilient against dead layers of Convolutions.\n",
    " * SENet: Added Squeeze and Excitation layers that inhibited information in one area.\n",
    " * Xception: Two types of Convolution layes, stacked strictly one on top of the other. The first looks for purely spatial information along a single channel (or a single feature map), while the second looks purely for non-spatial (1x1 filters) information across the channels (or feature maps)\n",
    " \n",
    " 7. A fully convolutional network has only convolution and max-pooling layers. Since these are indepedent of the input image size, they can be used to train larger images and produce the same predictive power as the final layer being a fully connected (dense) layer.\n",
    " \n",
    "To convert a dense final output layer with 200 neurons to a Fully Convolutional Network, you change the output layer to 200 layers each of 1x1 pixel, that are dependent on the bottleneck layer below it. The weights will be identical to the equivalent dense layer. And then each of these layers produces a probability of output $i$, unlike each neuron producing output $i$.\n",
    " \n",
    " 8. Semantic Segmentation requires labeled data, which is difficult. But the biggest difficulty is that CNNs lose spatial information, which is critical for semantic segmentation.\n",
    " \n",
    " 9. Build a CNN from scratch on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (55000, 28, 28)\n",
      "X_valid shape =  (5000, 28, 28)\n",
      "X_test shape =  (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import Chapter_10 as c10\n",
    "\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = c10.load_digits_mnist(debug=True)                                                \n",
    "\n",
    "def prepare_data(X_train, X_valid, X_test, debug=False):\n",
    "    if (debug):\n",
    "        print (\"X_train shape = \", X_train.shape)\n",
    "        print (\"X_valid shape = \", X_valid.shape)\n",
    "        print (\"X_test shape = \", X_test.shape)\n",
    "\n",
    "    X_train = X_train.reshape(55000,28,28,1)\n",
    "    X_valid = X_valid.reshape(5000,28,28,1)\n",
    "    X_test = X_test.reshape(10000,28,28,1)\n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "X_train, X_valid, X_test = prepare_data(X_train, X_valid, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1e57c0cc7590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Create the model and train it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"naive_deep_mnist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1e57c0cc7590>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, X_train, y_train, X_valid, y_valid, epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     history_conv = model.fit(x=X_train, y=y_train, batch_size=32, validation_data=[X_valid, y_valid],\n\u001b[0m\u001b[1;32m     42\u001b[0m                              epochs=epochs, verbose=0)\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory_conv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the deep convolutional model listed in Chapter 14, that is much better than the naive MNIST.\n",
    "# Let's see how good this is, since all the data is already loaded here.\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "def create_model(optimizer=\"sgd\"):\n",
    "    deep_model = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(32, 4, activation=\"relu\", padding=\"same\", \n",
    "                            input_shape=(28, 28, 1), name=\"input\"),\n",
    "        keras.layers.MaxPooling2D(1,name=\"firstPool\"),\n",
    "        keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\", \n",
    "                            name=\"first_conv_1\"),\n",
    "        keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\", \n",
    "                            name=\"first_conv_2\"),\n",
    "\n",
    "        keras.layers.MaxPooling2D(1, name=\"secondPool\"),\n",
    "        keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\", \n",
    "                            name=\"second_conv_1\"),\n",
    "        keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\", \n",
    "                            name=\"second_conv_2\"),\n",
    "\n",
    "        keras.layers.MaxPooling2D(1, name=\"thirdPool\"),\n",
    "\n",
    "        keras.layers.Flatten(name=\"flatten\"),\n",
    "        keras.layers.Dense(128, activation=\"relu\", name=\"pre-bottneck\"),\n",
    "\n",
    "        keras.layers.Dropout(0.5, name=\"bottleneckDropout\"),\n",
    "        keras.layers.Dense(64, activation=\"relu\", name=\"bottleneck\"),\n",
    "\n",
    "        keras.layers.Dropout(0.5, name=\"outputDropout\"),\n",
    "        keras.layers.Dense(10, activation=\"softmax\", name=\"output\"),\n",
    "    ])\n",
    "    \n",
    "    deep_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "    return deep_model\n",
    "\n",
    "def fit_model(model, X_train, y_train, X_valid, y_valid, epochs):\n",
    "    history_conv = model.fit(x=X_train, y=y_train, batch_size=32, validation_data=[X_valid, y_valid],\n",
    "                             epochs=epochs, verbose=0)\n",
    "    return history_conv\n",
    "\n",
    "def plot_history(history, name):\n",
    "    c10.plot_training(history, name, show=True)\n",
    "\n",
    "# Create the model and train it\n",
    "model = create_model()\n",
    "history = fit_model(model, X_train, y_train, X_valid, y_valid, epochs=10)\n",
    "plot_history(history, \"naive_deep_mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the problem with Tensorflow. The error startements are reminiscent of g++ in 2000. The errors you get are deep in the stack. It is literally the full call graph of the compiler flow with no real help for a user reading them.\n",
    "\n",
    "Here is a sample output. Quick, what is this failure?\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-24-88fd4d8bab85> in <module>\n",
    "     41 \n",
    "     42 model = create_model()\n",
    "---> 43 history = fit_model(model, X_train, y_train, X_valid, y_valid, epochs=10)\n",
    "     44 plot_history(history, \"naive_deep_mnist\")\n",
    "\n",
    "<ipython-input-24-88fd4d8bab85> in fit_model(model, X_train, y_train, X_valid, y_valid, epochs)\n",
    "     34 \n",
    "     35 def fit_model(model, X_train, y_train, X_valid, y_valid, epochs):\n",
    "---> 36     history_conv = model.fit(X_train, y_train, validation_data=[X_valid, y_valid], epochs=epochs, verbose=0)\n",
    "     37     return history_conv\n",
    "     38 \n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\n",
    "    106   def _method_wrapper(self, *args, **kwargs):\n",
    "    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\n",
    "--> 108       return method(self, *args, **kwargs)\n",
    "    109 \n",
    "    110     # Running inside `run_distribute_coordinator` already.\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\n",
    "   1096                 batch_size=batch_size):\n",
    "   1097               callbacks.on_train_batch_begin(step)\n",
    "-> 1098               tmp_logs = train_function(iterator)\n",
    "   1099               if data_handler.should_sync:\n",
    "   1100                 context.async_wait()\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\n",
    "    778       else:\n",
    "    779         compiler = \"nonXla\"\n",
    "--> 780         result = self._call(*args, **kwds)\n",
    "    781 \n",
    "    782       new_tracing_count = self._get_tracing_count()\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\n",
    "    821       # This is the first call of __call__, so we have to initialize.\n",
    "    822       initializers = []\n",
    "--> 823       self._initialize(args, kwds, add_initializers_to=initializers)\n",
    "    824     finally:\n",
    "    825       # At this point we know that the initialization is complete (or less\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\n",
    "    694     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)\n",
    "    695     self._concrete_stateful_fn = (\n",
    "--> 696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n",
    "    697             *args, **kwds))\n",
    "    698 \n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\n",
    "   2853       args, kwargs = None, None\n",
    "   2854     with self._lock:\n",
    "-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\n",
    "   2856     return graph_function\n",
    "   2857 \n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\n",
    "   3211 \n",
    "   3212       self._function_cache.missed.add(call_context_key)\n",
    "-> 3213       graph_function = self._create_graph_function(args, kwargs)\n",
    "   3214       self._function_cache.primary[cache_key] = graph_function\n",
    "   3215       return graph_function, args, kwargs\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\n",
    "   3063     arg_names = base_arg_names + missing_arg_names\n",
    "   3064     graph_function = ConcreteFunction(\n",
    "-> 3065         func_graph_module.func_graph_from_py_func(\n",
    "   3066             self._name,\n",
    "   3067             self._python_function,\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\n",
    "    984         _, original_func = tf_decorator.unwrap(python_func)\n",
    "    985 \n",
    "--> 986       func_outputs = python_func(*func_args, **func_kwargs)\n",
    "    987 \n",
    "    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\n",
    "    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\n",
    "    599         # the function a weak reference to itself to avoid a reference cycle.\n",
    "--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
    "    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\n",
    "    602 \n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\n",
    "    971           except Exception as e:  # pylint:disable=broad-except\n",
    "    972             if hasattr(e, \"ag_error_metadata\"):\n",
    "--> 973               raise e.ag_error_metadata.to_exception(e)\n",
    "    974             else:\n",
    "    975               raise\n",
    "\n",
    "ValueError: in user code:\n",
    "\n",
    "    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n",
    "        return step_function(self, iterator)\n",
    "    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n",
    "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
    "    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n",
    "        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
    "    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n",
    "        return self._call_for_each_replica(fn, args, kwargs)\n",
    "    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n",
    "        return fn(*args, **kwargs)\n",
    "    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n",
    "        outputs = model.train_step(data)\n",
    "    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:747 train_step\n",
    "        y_pred = self(x, training=True)\n",
    "    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py:975 __call__\n",
    "        input_spec.assert_input_compatibility(self.input_spec, inputs,\n",
    "    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/input_spec.py:191 assert_input_compatibility\n",
    "        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n",
    "\n",
    "    ValueError: Input 0 of layer sequential_15 is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: [None, 28, 28]\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "I am sure that an experienced Tensorflow user can find out exactly what the problem is. That is exactly the point. I'm trying to construct a deep CNN, using some rudimentary Python code. This is the full code:\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "def create_model(optimizer=\"sgd\"):\n",
    "    deep_model = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(64, 7, activation=\"relu\", padding=\"same\", input_shape=[1, 28, 28]),\n",
    "        keras.layers.MaxPooling2D(1),\n",
    "        keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "        keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "\n",
    "        keras.layers.MaxPooling2D(1),\n",
    "        keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "        keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "\n",
    "        keras.layers.MaxPooling2D(1),\n",
    "\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation=\"relu\"),\n",
    "\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "    \n",
    "    deep_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "    return deep_model\n",
    "\n",
    "def fit_model(model, X_train, y_train, X_valid, y_valid, epochs):\n",
    "    history_conv = model.fit(X_train, y_train, validation_data=[X_valid, y_valid], epochs=epochs, verbose=0)\n",
    "    return history_conv\n",
    "\n",
    "def plot_history(history, name):\n",
    "    c10.plot_training(history, name, show=True)\n",
    "    \n",
    "model = create_model()\n",
    "history = fit_model(model, X_train, y_train, X_valid, y_valid, epochs=10)\n",
    "plot_history(history, \"naive_deep_mnist\")\n",
    "```\n",
    "\n",
    "When the full error is deeper and more inscrutable than the full code, the tool is not helpful. It might as well say, \"You've got a problem somewhere.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here's another error:\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-25-8233eab5f9a3> in <module>\n",
    "     40     c10.plot_training(history, name, show=True)\n",
    "     41 \n",
    "---> 42 model = create_model()\n",
    "     43 history = fit_model(model, X_train, y_train, X_valid, y_valid, epochs=10)\n",
    "     44 plot_history(history, \"naive_deep_mnist\")\n",
    "\n",
    "<ipython-input-25-8233eab5f9a3> in create_model(optimizer)\n",
    "      5 \n",
    "      6 def create_model(optimizer=\"sgd\"):\n",
    "----> 7     deep_model = keras.models.Sequential([\n",
    "      8         keras.layers.Conv2D(64, 7, activation=\"relu\", padding=\"same\", input_shape=[1, 28, 28], name=\"input\"),\n",
    "      9         keras.layers.MaxPooling2D(1,name=\"firstPool\"),\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\n",
    "    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\n",
    "    456     try:\n",
    "--> 457       result = method(self, *args, **kwargs)\n",
    "    458     finally:\n",
    "    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)\n",
    "    140         layers = [layers]\n",
    "    141       for layer in layers:\n",
    "--> 142         self.add(layer)\n",
    "    143 \n",
    "    144   @property\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\n",
    "    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\n",
    "    456     try:\n",
    "--> 457       result = method(self, *args, **kwargs)\n",
    "    458     finally:\n",
    "    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/sequential.py in add(self, layer)\n",
    "    219       # If the model is being built continuously on top of an input layer:\n",
    "    220       # refresh its output.\n",
    "--> 221       output_tensor = layer(self.outputs[0])\n",
    "    222       if len(nest.flatten(output_tensor)) != 1:\n",
    "    223         raise ValueError(SINGLE_LAYER_OUTPUT_ERROR_MSG)\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\n",
    "    923     # >> model = tf.keras.Model(inputs, outputs)\n",
    "    924     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):\n",
    "--> 925       return self._functional_construction_call(inputs, args, kwargs,\n",
    "    926                                                 input_list)\n",
    "    927 \n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)\n",
    "   1115           try:\n",
    "   1116             with ops.enable_auto_cast_variables(self._compute_dtype_object):\n",
    "-> 1117               outputs = call_fn(cast_inputs, *args, **kwargs)\n",
    "   1118 \n",
    "   1119           except errors.OperatorNotAllowedInGraphError as e:\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/layers/core.py in call(self, inputs, training)\n",
    "    214           rate=self.rate)\n",
    "    215 \n",
    "--> 216     output = tf_utils.smart_cond(training,\n",
    "    217                                  dropped_inputs,\n",
    "    218                                  lambda: array_ops.identity(inputs))\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/tf_utils.py in smart_cond(pred, true_fn, false_fn, name)\n",
    "     62     return control_flow_ops.cond(\n",
    "     63         pred, true_fn=true_fn, false_fn=false_fn, name=name)\n",
    "---> 64   return smart_module.smart_cond(\n",
    "     65       pred, true_fn=true_fn, false_fn=false_fn, name=name)\n",
    "     66 \n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)\n",
    "     56       return false_fn()\n",
    "     57   else:\n",
    "---> 58     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\n",
    "     59                                  name=name)\n",
    "     60 \n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\n",
    "    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\n",
    "    200     try:\n",
    "--> 201       return target(*args, **kwargs)\n",
    "    202     except (TypeError, ValueError):\n",
    "    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\n",
    "    505                 'in a future version' if date is None else ('after %s' % date),\n",
    "    506                 instructions)\n",
    "--> 507       return func(*args, **kwargs)\n",
    "    508 \n",
    "    509     doc = _add_deprecated_arg_notice_to_docstring(\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/control_flow_ops.py in cond(pred, true_fn, false_fn, strict, name, fn1, fn2)\n",
    "   1178   if (util.EnableControlFlowV2(ops.get_default_graph()) and\n",
    "   1179       not context.executing_eagerly()):\n",
    "-> 1180     return cond_v2.cond_v2(pred, true_fn, false_fn, name)\n",
    "   1181 \n",
    "   1182   # We needed to make true_fn/false_fn keyword arguments for\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/cond_v2.py in cond_v2(pred, true_fn, false_fn, name)\n",
    "     77       pred = array_ops.squeeze_v2(pred)\n",
    "     78 \n",
    "---> 79     true_graph = func_graph_module.func_graph_from_py_func(\n",
    "     80         true_name,\n",
    "     81         true_fn, [], {},\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\n",
    "    984         _, original_func = tf_decorator.unwrap(python_func)\n",
    "    985 \n",
    "--> 986       func_outputs = python_func(*func_args, **func_kwargs)\n",
    "    987 \n",
    "    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/layers/core.py in dropped_inputs()\n",
    "    208 \n",
    "    209     def dropped_inputs():\n",
    "--> 210       return nn.dropout(\n",
    "    211           inputs,\n",
    "    212           noise_shape=self._get_noise_shape(inputs),\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\n",
    "    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\n",
    "    200     try:\n",
    "--> 201       return target(*args, **kwargs)\n",
    "    202     except (TypeError, ValueError):\n",
    "    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\n",
    "    505                 'in a future version' if date is None else ('after %s' % date),\n",
    "    506                 instructions)\n",
    "--> 507       return func(*args, **kwargs)\n",
    "    508 \n",
    "    509     doc = _add_deprecated_arg_notice_to_docstring(\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/nn_ops.py in dropout(x, keep_prob, noise_shape, seed, name, rate)\n",
    "   4940     raise ValueError(\"You must provide a rate to dropout.\")\n",
    "   4941 \n",
    "-> 4942   return dropout_v2(x, rate, noise_shape=noise_shape, seed=seed, name=name)\n",
    "   4943 \n",
    "   4944 \n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\n",
    "    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\n",
    "    200     try:\n",
    "--> 201       return target(*args, **kwargs)\n",
    "    202     except (TypeError, ValueError):\n",
    "    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/nn_ops.py in dropout_v2(x, rate, noise_shape, seed, name)\n",
    "   5054     # NOTE: Random uniform can only generate 2^23 floats on [1.0, 2.0)\n",
    "   5055     # and subtract 1.0.\n",
    "-> 5056     random_tensor = random_ops.random_uniform(\n",
    "   5057         noise_shape, seed=seed, dtype=x_dtype)\n",
    "   5058     # NOTE: if (1.0 + rate) - 1 is equal to rate, then that float is selected,\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\n",
    "    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\n",
    "    200     try:\n",
    "--> 201       return target(*args, **kwargs)\n",
    "    202     except (TypeError, ValueError):\n",
    "    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/random_ops.py in random_uniform(shape, minval, maxval, dtype, seed, name)\n",
    "    299           shape, minval, maxval, seed=seed1, seed2=seed2, name=name)\n",
    "    300     else:\n",
    "--> 301       result = gen_random_ops.random_uniform(\n",
    "    302           shape, dtype, seed=seed1, seed2=seed2)\n",
    "    303       if minval_is_zero:\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_random_ops.py in random_uniform(shape, dtype, seed, seed2, name)\n",
    "    740     seed2 = 0\n",
    "    741   seed2 = _execute.make_int(seed2, \"seed2\")\n",
    "--> 742   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
    "    743         \"RandomUniform\", shape=shape, dtype=dtype, seed=seed, seed2=seed2,\n",
    "    744                          name=name)\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\n",
    "    574         else:\n",
    "    575           for base_type in base_types:\n",
    "--> 576             _SatisfiesTypeConstraint(base_type,\n",
    "    577                                      _Attr(op_def, input_arg.type_attr),\n",
    "    578                                      param_name=input_name)\n",
    "\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)\n",
    "     55     allowed_list = attr_def.allowed_values.list.type\n",
    "     56     if dtype not in allowed_list:\n",
    "---> 57       raise TypeError(\n",
    "     58           \"Value passed to parameter '%s' has DataType %s not in list of \"\n",
    "     59           \"allowed values: %s\" %\n",
    "\n",
    "TypeError: Value passed to parameter 'shape' has DataType string not in list of allowed values: int32, int64\n",
    "\n",
    "\n",
    "```\n",
    "created by this code\n",
    "```\n",
    "# Create the deep convolutional model listed in Chapter 14, that is much better than the naive MNIST.\n",
    "# Let's see how good this is, since all the data is already loaded here.\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "def create_model(optimizer=\"sgd\"):\n",
    "    deep_model = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(64, 7, activation=\"relu\", padding=\"same\", input_shape=[1, 28, 28], name=\"input\"),\n",
    "        keras.layers.MaxPooling2D(1,name=\"firstPool\"),\n",
    "        keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\", name=\"first_conv_1\"),\n",
    "        keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\", name=\"first_conv_2\"),\n",
    "\n",
    "        keras.layers.MaxPooling2D(1, name=\"secondPool\"),\n",
    "        keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\", name=\"second_conv_1\"),\n",
    "        keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\", name=\"second_conv_2\"),\n",
    "\n",
    "        keras.layers.MaxPooling2D(1, name=\"thirdPool\"),\n",
    "\n",
    "        keras.layers.Flatten(name=\"flatten\"),\n",
    "        keras.layers.Dense(128, activation=\"relu\", name=\"pre-bottneck\"),\n",
    "\n",
    "        keras.layers.Dropout(0.5, \"bottleneckDropout\"),\n",
    "        keras.layers.Dense(64, activation=\"relu\", name=\"bottleneck\"),\n",
    "\n",
    "        keras.layers.Dropout(0.5, \"outputDropout\"),\n",
    "        keras.layers.Dense(10, activation=\"softmax\", name=\"output\"),\n",
    "    ])\n",
    "    \n",
    "    deep_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "    return deep_model\n",
    "\n",
    "def fit_model(model, X_train, y_train, X_valid, y_valid, epochs):\n",
    "    history_conv = model.fit(X_train, y_train, validation_data=[X_valid, y_valid], epochs=epochs, verbose=0)\n",
    "    return history_conv\n",
    "\n",
    "def plot_history(history, name):\n",
    "    c10.plot_training(history, name, show=True)\n",
    "    \n",
    "model = create_model()\n",
    "history = fit_model(model, X_train, y_train, X_valid, y_valid, epochs=10)\n",
    "plot_history(history, \"naive_deep_mnist\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
