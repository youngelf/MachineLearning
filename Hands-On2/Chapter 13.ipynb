{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Loading and Preprocessing data\n",
    "\n",
    "Not much to follow along, as the examples in the book are all snippets of data rather than a full run.\n",
    "\n",
    "The exercises will be the bulk of the learning.\n",
    "\n",
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the data API to read model data, in a repeatable and fast way. Also, to repeat or shuffle the input data, get smaller samples of the data, and to cache disk-based data in RAM for faster access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Large datasets in different files can allow multiple processes to read different files in parallel, with each process/thread acting on one subset of the input data for map-reduce like semantics.\n",
    "\n",
    " Also, it allows the data to be read by multiple machines.\n",
    " \n",
    " Finally, it might be the only way to read data, if the dataset is larger than RAM: to break it into RAM-sized chunks, and read from each file, act upon the data, evict the memory, and then move to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You can instrument the time spent in reading the data versus computing the models. In many naive cases, folks spend a lot of time reading data from the cloud (like [this example](https://towardsdatascience.com/music-genre-classification-with-tensorflow-3de38f0d4dbb)) rather than model training.  This is also the case if the model is built and preprocessing of the incoming streaming data is taking longer than inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Only serialized protobufs can be saved in TFRecord files.\n",
    "\n",
    "(..is my guess). Let's check the book. The book is silent on the details, but it is basically a protobuf written to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. If we had our own protobuf format, we'd have to write reader and writer methods. It is definitely possible, but much easier to use the TF format, as they are written for most common data and dataset operations. This makes it much easier to read the serialized formats directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. I'd use compression when:\n",
    "  * Using or transporting large files\n",
    "  * Sending data over the wire is prohibitive\n",
    "  * When the underlying data is relatively sparse (large empty files)\n",
    "  * When CPU is cheap but disk or network is expensive\n",
    "  \n",
    " I wouldn't use compression when:\n",
    "  * Data is small\n",
    "  * Data is meant to be stored on disk, and not sent over networks.\n",
    "  * Data can be generated trivially\n",
    "  * Data is information-rich, like zip files, or mp3 or jpg files which compress poorly.\n",
    "  * CPU is at a premium and disk is cheap.\n",
    "    \n",
    "I wouldn't use compression all the time. For small files, compression doesn't add much help and might even increase the size of the data, or the CPU time used (or both)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Writing data files:\n",
    "Pros:\n",
    "  * When data is generated outside of tensorflow (server producing log files)\n",
    "  * Data is ancient and exists, and needs to be maintained as-is.\n",
    "  * The ML work is speculative, and might not yield benefit.\n",
    "  * Persistent\n",
    "  \n",
    "Cons:\n",
    "  * Difficult to work with. You need to know what the data format is.\n",
    "  * Can be inefficient to store on disk.\n",
    "  * Has OS level file-system quirks. Files might be case sensitive (Linux), not be case sensitive (Windows) or case sensitive, but in a way that nobody understands (Mac)\n",
    "  * Can get mangled without anyone noticing. The end of the file might get truncated during transfer, for example.\n",
    "  * Rely on endian-ness, or character encoding (Unicode, UTF-8), or precision (32-bit FP?, 64-bit FP? 38-bit?)\n",
    "  * Outside of normal TF world: training, fitting, etc.\n",
    "    \n",
    "TF-data pipeline:\n",
    "Pros:\n",
    "  * A standard mechanism.\n",
    "  * Works great across platforms.\n",
    "  \n",
    "\n",
    "Cons:\n",
    "  * Memory only\n",
    "  \n",
    "Processing layer within model.\n",
    "Pros:\n",
    "  * Integrated within the normal training.\n",
    "  * Reliable, can be used for both training and inference\n",
    "  \n",
    "Cons:\n",
    "  * Not persistent.\n",
    "  \n",
    "TF Transform:\n",
    "Pros:\n",
    "  * Can be deterministically and correctly applied both during training and inference.\n",
    "  * Can be used outside of TensorFlow for larger datasets in a parallel environment.\n",
    "  \n",
    "  \n",
    "Cons:\n",
    "  * Just for processing, not for creating/storing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Encoding categorical features. I'd either use:\n",
    "  * String enums where the values have some meaning, and then export them as one-hot vectors. For example the species label of an observation.\n",
    "  * One-hot vectors when they don't have a simple English meaning. For example, form number: 1040-R versus 1040-EZ.\n",
    "  * Integers, when they can be encoded: ZIP code, which don't have ordinal information (94303 is not *more* than 94043)\n",
    "\n",
    "\n",
    "Text can be encoded as:\n",
    "  * Sparse vectors of occurrance for a bag-of-words model: words in a news-article, for example.\n",
    "  * One-hot vector when you only get a single value: \"single-family home\" versus \"apartment\" versus \"empty-lot\" for housing, for example.\n",
    "  * integers/floats when the words can be organized in some ordinal sense (colors of the rainbow can become a frequency.\n",
    "  * Just text when they are unique identifiers: first-name, last-name of people,\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Load Fashion mnist and split and save each dataset to multiple tfrecord files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version  2.3.0\n",
      "Keras version  2.4.0\n"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.image import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import sys\n",
    "\n",
    "print(\"TF version \", tf.__version__)\n",
    "print(\"Keras version \", keras.__version__)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaded Chapter_10.py. So nice to have actual methods rather than the mess that is jupyter's code layout.\n",
    "sys.path.insert(0, '/home/viki/ml/MachineLearning/Hands-On2')\n",
    "\n",
    "# Change these to Chapter_10 hereafter once the conversion is complete and the sys.exit(0) is gone.\n",
    "# import Chapter_10\n",
    "import c10 as Chapter_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test, y_train, y_valid, y_test, class_names = Chapter_10.load_fashion_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
