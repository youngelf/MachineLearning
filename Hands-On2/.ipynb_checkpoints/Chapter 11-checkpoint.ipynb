{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Training deep neural networks\n",
    "\n",
    "\n",
    "Neural networks suffer from vanishing (too small) or exploding (too large) gradients.\n",
    "This caused them to be abandoned.\n",
    "\n",
    "Xavier Glorot and Yoshua Bengio showed in 2010 that this was caused by logistic activation (mean is 0.5, and gradient is close to the edges: output 0 or output 1) or the initialization used.\n",
    "\n",
    "The outcome was to have a fan-in as close to fan-out for hidden layers, and the initialization is done in a crafty way to account for differences in fan-in and fan-out called Xavier initialization now. $\\sigma^2 = 1/{fan}$, fan is the average of fan-in and fan-out.\n",
    "\n",
    "\n",
    "Another approach is to use Rectified Linear Units (ReLU) with another kind of initialization called He Initialization. $\\sigma^2 = 2/{fan}_{in}$\n",
    "\n",
    "A third approach is Scaled Exponential Linear Units (SELU) with a third initialization mechanism called LeCunn Initialization. $\\sigma^2 = 1/{fan}_{in}$\n",
    "\n",
    "This can be modified in Keras by doing kernel_initializer=\"he_uniform\" or \"he_normal\" etc.\n",
    "\n",
    "The 2010 paper showed that just because biological neurons use logistic activation, we don't have to. And in fact, using them causes all sorts of mathematical problems. ReLU activation functions work well in practice, but suffer from a problem where once they start outputting 0, they stay there. As a result, a leaky ReLU works better when it outputs a small negative value instead of 0 all the way through.\n",
    "\n",
    "The exponential LU is an exponential function shifted down by 1, so it outputs -1 (instead of 0 at $-\\inf$ and 0 at 1): ${ELU}(z) = exp(z) - 1$. Differentiable, doesn't cause vanishing or exploding gradients. Slower to compute\n",
    "\n",
    "SELU needs sequential networks (no skip connections). Needs a specific initializer \"lecun_normal\", and standard scaling of inputs. When this happens, SELU will self-normalize (mean 0 and variance 1, this is desirable) when all hidden layers use SELU.\n",
    "\n",
    "\n",
    "Initialization choices listed above only help at the start of the training. During training, the intermediate layers can still have poor gradients. Batch Normalization is a set of extra layers added that seek to estimate the mean and variance of their inputs at their layer (during training), and modify output scaling and output shifting as errors are calculated. After training, the layer modifies its inputs by using the training means and variances, and also modifies the output to scale and shift it to ensure that the behavior at that layer is good.\n",
    "\n",
    "I don't have a good sense of it though. To standard-scale the input, it must be applied before a hidden neural network layer, and to modify the output, it should be applied after the neural network layer. Not sure how this really works in practice.\n",
    "\n",
    "\n",
    "This chapter was mostly talk. The exercises are where the information get solidified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. It is ok to initialize values to the same thing, though you are better off with random initialization to different values.\n",
    "\n",
    "2. Nope, not ok to initialize values to 0. Some of the activation functions have a zero gradient (or undefined gradient) at 0. Best to initialize to nonzero values.\n",
    "\n",
    "3. SELU is differentiable everywhere, and usually converges faster, even though it is slower to compute.\n",
    " When SELU is used in all layers, it self-normalizes to mean 0 and standard deviation of 1, which greatly helps convergence.\n",
    "\n",
    "4. SELU: when the input can be scaled, and \n",
    "\n",
    "5. No idea. Probably the result doesn't converge?\n",
    "\n",
    "6. Sparse models can be produced by:\n",
    "  * Using dropout. This removes some nodes.\n",
    "  * High value of regularization.\n",
    "  * Ue Tensorflow's Model Optimization Toolkit (MOT) to prune connections with small magnitude.\n",
    "\n",
    "7. Dropout might down training as we might need more iterations to converge. It does speeds up inference. MC Dropout slows down training as we have to get the boosted model iteratively. And it does slow down inference as well (inference requires training with dropout turned on and keeping the previous inferred results to average)\n",
    "\n",
    "8. Doing that below here.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version  2.3.0\n",
      "Keras version  2.4.0\n"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.image import imread\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TF version \", tf.__version__)\n",
    "print(\"Keras version \", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (testX, testy) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 100)               10100     \n",
      "=================================================================\n",
      "Total params: 499,200\n",
      "Trainable params: 499,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def create_keras_classifier_model(n_classes):\n",
    "    \"\"\"Keras multinomial logistic regression creation model\n",
    " \n",
    "    Args:\n",
    "        n_classes(int): Number of classes to be classified\n",
    " \n",
    "    Returns:\n",
    "        Compiled keras model\n",
    " \n",
    "    \"\"\"\n",
    "    # create model\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "    for i in range(20):\n",
    "        model.add(keras.layers.Dense(\n",
    "            n_classes, \n",
    "            activation=\"elu\",\n",
    "            kernel_initializer=tf.keras.initializers.HeNormal()\n",
    "        ))\n",
    "\n",
    "    print(model.summary())\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", \n",
    "        optimizer=\"adam\", \n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    " \n",
    "estimator = keras.wrappers.scikit_learn.KerasClassifier(\n",
    "    build_fn=create_keras_classifier_model,\n",
    "    n_classes=10,\n",
    "    class_weight={0: 1, 1:3})\n",
    "\n",
    "viki_stack_trace = ''\n",
    "\n",
    "q = create_keras_classifier_model(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to create a model and test against the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
