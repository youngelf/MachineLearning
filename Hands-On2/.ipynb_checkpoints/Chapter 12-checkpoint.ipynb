{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Custom Models and Training with Tensorflow\n",
    "\n",
    "\n",
    "\n",
    "Creating a new loss function allows you to store the config, load from a config and apply ('call') the method.\n",
    "\n",
    "Initializers, Regularizers, Constraings can be overwriten. A kernel_constraint allows you to overwrite the edge\n",
    "weights \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version  2.3.0\n",
      "Keras version  2.4.0\n"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.image import imread\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TF version \", tf.__version__)\n",
    "print(\"Keras version \", keras.__version__)\n",
    "\n",
    "# Custom error handler for the entire notebook so stack traces are not lost\n",
    "from IPython.core.ultratb import AutoFormattedTB\n",
    "\n",
    "# initialize the formatter for making the tracebacks into strings\n",
    "itb = AutoFormattedTB(mode = 'Plain', tb_offset = 1)\n",
    "\n",
    "# Define a global with the stack trace that we can append to in the handler.\n",
    "viki_stack_trace = ''\n",
    "\n",
    "# this function will be called on exceptions in any cell\n",
    "def custom_exc(shell, etype, evalue, tb, tb_offset=None):\n",
    "    global viki_stack_trace\n",
    "\n",
    "    # still show the error within the notebook, don't just swallow it\n",
    "    shell.showtraceback((etype, evalue, tb), tb_offset=tb_offset)\n",
    "\n",
    "    # grab the traceback and make it into a list of strings\n",
    "    stb = itb.structured_traceback(etype, evalue, tb)\n",
    "    sstb = itb.stb2text(stb)\n",
    "\n",
    "    print (sstb) # <--- this is the variable with the traceback string\n",
    "    viki_stack_trace = viki_stack_trace + sstb\n",
    "\n",
    "# this registers a custom exception handler for the whole current notebook\n",
    "get_ipython().set_custom_exc((Exception,), custom_exc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Loss\" functions are used during training, and their gradient is what is optimized.\n",
    "\n",
    "By contrast, \"metrics\" are used to evaluate a model, they can be anything arbitrary. They have no expectation of\n",
    "having nonzero values or existence of gradients.\n",
    "\n",
    "This is a custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    \"A custom loss function that will be used later. Just an example\"\n",
    "\n",
    "    def __init(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"Evaluate the loss at this stage\"\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold ** 2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"Called when model is saved to preserve existing config. This class will save its parent class' config too.\"\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are other custom functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_softplus(z):\n",
    "    \"Used to return a probability of seeing this output\"\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def initializer_glorot(shape, dtype=tf.float32):\n",
    "    \"Used to initialize weights before training\"\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def regularizer_l1(weights):\n",
    "    \"Used to avoid over-fitting, and keep weights meaningful\"\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def constraint_weights(weights):\n",
    "    \"Applied after the training to constrain the weights at the layer arbitrarily\"\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above methods can be used directly, but we can also create a class that inherits from\n",
    "keras.initializers.Initializer, keras.regularizers.Regularizer, and keras.constraints.Constraint appropriately.\n",
    "The activation function usually has nothing to save, so if you want to have a parameter for the activation, you can create a new layer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VikiL1(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        \"Create a regularizer with L1 regularization and the factor provided here\"\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        \"Apply this regularizer with the weights at this layer\"\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"Returns the configuration of this class for application later\"\n",
    "        return {\"factor\": self.factor} # We don't look up the parent's config, because it has none.\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
