{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Training deep neural networks\n",
    "\n",
    "\n",
    "Neural networks suffer from vanishing (too small) or exploding (too large) gradients.\n",
    "This caused them to be abandoned.\n",
    "\n",
    "Xavier Glorot and Yoshua Bengio showed in 2010 that this was caused by logistic activation (mean is 0.5, and gradient is close to the edges: output 0 or output 1) or the initialization used.\n",
    "\n",
    "The outcome was to have a fan-in as close to fan-out for hidden layers, and the initialization is done in a crafty way to account for differences in fan-in and fan-out called Xavier initialization now. $\\sigma^2 = 1/{fan}$, fan is the average of fan-in and fan-out.\n",
    "\n",
    "\n",
    "Another approach is to use Rectified Linear Units (ReLU) with another kind of initialization called He Initialization. $\\sigma^2 = 2/{fan}_{in}$\n",
    "\n",
    "A third approach is Scaled Exponential Linear Units (SELU) with a third initialization mechanism called LeCunn Initialization. $\\sigma^2 = 1/{fan}_{in}$\n",
    "\n",
    "This can be modified in Keras by doing kernel_initializer=\"he_uniform\" or \"he_normal\" etc.\n",
    "\n",
    "The 2010 paper showed that just because biological neurons use logistic activation, we don't have to. And in fact, using them causes all sorts of mathematical problems. ReLU activation functions work well in practice, but suffer from a problem where once they start outputting 0, they stay there. As a result, a leaky ReLU works better when it outputs a small negative value instead of 0 all the way through.\n",
    "\n",
    "The exponential LU is an exponential function shifted down by 1, so it outputs -1 (instead of 0 at $-\\inf$ and 0 at 1): ${ELU}(z) = exp(z) - 1$. Differentiable, doesn't cause vanishing or exploding gradients. Slower to compute\n",
    "\n",
    "SELU needs sequential networks (no skip connections). Needs a specific initializer \"lecun_normal\", and standard scaling of inputs. When this happens, SELU will self-normalize (mean 0 and variance 1, this is desirable) when all hidden layers use SELU.\n",
    "\n",
    "\n",
    "Initialization choices listed above only help at the start of the training. During training, the intermediate layers can still have poor gradients. Batch Normalization is a set of extra layers added that seek to estimate the mean and variance of their inputs at their layer (during training), and modify output scaling and output shifting as errors are calculated. After training, the layer modifies its inputs by using the training means and variances, and also modifies the output to scale and shift it to ensure that the behavior at that layer is good.\n",
    "\n",
    "I don't have a good sense of it though. To standard-scale the input, it must be applied before a hidden neural network layer, and to modify the output, it should be applied after the neural network layer. Not sure how this really works in practice.\n",
    "\n",
    "\n",
    "This chapter was mostly talk. The exercises are where the information get solidified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. It is ok to initialize values to the same thing, though you are better off with random initialization to different values.\n",
    "\n",
    "2. Nope, not ok to initialize values to 0. Some of the activation functions have a zero gradient (or undefined gradient) at 0. Best to initialize to nonzero values.\n",
    "\n",
    "3. SELU is differentiable everywhere, and usually converges faster, even though it is slower to compute.\n",
    " When SELU is used in all layers, it self-normalizes to mean 0 and standard deviation of 1, which greatly helps convergence.\n",
    "\n",
    "4. SELU: when the input can be scaled, and \n",
    "\n",
    "5. No idea. Probably the result doesn't converge?\n",
    "\n",
    "6. Sparse models can be produced by:\n",
    "  * Using dropout. This removes some nodes.\n",
    "  * High value of regularization.\n",
    "  * Ue Tensorflow's Model Optimization Toolkit (MOT) to prune connections with small magnitude.\n",
    "\n",
    "7. Dropout might down training as we might need more iterations to converge. It does speeds up inference. MC Dropout slows down training as we have to get the boosted model iteratively. And it does slow down inference as well (inference requires training with dropout turned on and keeping the previous inferred results to average)\n",
    "\n",
    "8. Doing that below here.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version  2.3.0\n",
      "Keras version  2.4.0\n"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.image import imread\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TF version \", tf.__version__)\n",
    "print(\"Keras version \", keras.__version__)\n",
    "\n",
    "# Custom error handler for the entire notebook so stack traces are not lost\n",
    "from IPython.core.ultratb import AutoFormattedTB\n",
    "\n",
    "# initialize the formatter for making the tracebacks into strings\n",
    "itb = AutoFormattedTB(mode = 'Plain', tb_offset = 1)\n",
    "\n",
    "# Define a global with the stack trace that we can append to in the handler.\n",
    "viki_stack_trace = ''\n",
    "\n",
    "# this function will be called on exceptions in any cell\n",
    "def custom_exc(shell, etype, evalue, tb, tb_offset=None):\n",
    "    global viki_stack_trace\n",
    "\n",
    "    # still show the error within the notebook, don't just swallow it\n",
    "    shell.showtraceback((etype, evalue, tb), tb_offset=tb_offset)\n",
    "\n",
    "    # grab the traceback and make it into a list of strings\n",
    "    stb = itb.structured_traceback(etype, evalue, tb)\n",
    "    sstb = itb.stb2text(stb)\n",
    "\n",
    "    print (sstb) # <--- this is the variable with the traceback string\n",
    "    viki_stack_trace = viki_stack_trace + sstb\n",
    "\n",
    "# this registers a custom exception handler for the whole current notebook\n",
    "get_ipython().set_custom_exc((Exception,), custom_exc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (testX, testy) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_182 (Dense)            (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_184 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_185 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_188 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_189 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_190 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_191 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_192 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_195 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_199 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_200 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_201 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_202 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def create_keras_classifier_model(n_classes=100):\n",
    "    \"\"\"Keras multinomial logistic regression creation model\n",
    " \n",
    "    Args:\n",
    "        n_classes(int): Number of classes to be classified\n",
    " \n",
    "    Returns:\n",
    "        Compiled keras model\n",
    " \n",
    "    \"\"\"\n",
    "    # create model\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # The input: we get 32x32 pixels, each with 3 colors (rgb)\n",
    "    model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "    # Then the hidden layers, fully connected (100 by default)\n",
    "    for i in range(20):\n",
    "        model.add(keras.layers.Dense(\n",
    "            n_classes, \n",
    "            activation=\"elu\",\n",
    "            kernel_initializer=tf.keras.initializers.HeNormal()\n",
    "        ))\n",
    "    # Now add the output layer: 10 classes in CIFAR10, so 10 outputs.\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    print(model.summary())\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", \n",
    "        optimizer=\"nadam\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    " \n",
    "estimator = keras.wrappers.scikit_learn.KerasClassifier(\n",
    "    build_fn=create_keras_classifier_model,\n",
    "    n_classes=10,\n",
    "    class_weight={0: 1, 1:3})\n",
    "\n",
    "viki_stack_trace = ''\n",
    "\n",
    "mm = create_keras_classifier_model(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to create a model and test against the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 3.3307 - accuracy: 0.2220\n",
      "Epoch 2/30\n",
      "1563/1563 [==============================] - 85s 54ms/step - loss: 1.9199 - accuracy: 0.2920\n",
      "Epoch 3/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.8658 - accuracy: 0.3137\n",
      "Epoch 4/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.8350 - accuracy: 0.3299\n",
      "Epoch 5/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.7982 - accuracy: 0.3459\n",
      "Epoch 6/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.7787 - accuracy: 0.3539\n",
      "Epoch 7/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.7599 - accuracy: 0.3602\n",
      "Epoch 8/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 2.5332 - accuracy: 0.3436\n",
      "Epoch 9/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 2.0148 - accuracy: 0.2232\n",
      "Epoch 10/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.8990 - accuracy: 0.2870\n",
      "Epoch 11/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.8506 - accuracy: 0.3102\n",
      "Epoch 12/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.7974 - accuracy: 0.3329\n",
      "Epoch 13/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.7717 - accuracy: 0.3433\n",
      "Epoch 14/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.7509 - accuracy: 0.3568\n",
      "Epoch 15/30\n",
      "1563/1563 [==============================] - 85s 54ms/step - loss: 1.7293 - accuracy: 0.3677\n",
      "Epoch 16/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.7155 - accuracy: 0.3744\n",
      "Epoch 17/30\n",
      "1563/1563 [==============================] - 85s 54ms/step - loss: 1.7035 - accuracy: 0.3797\n",
      "Epoch 18/30\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.6904 - accuracy: 0.3864\n",
      "Epoch 19/30\n",
      "1563/1563 [==============================] - 89s 57ms/step - loss: 1.6830 - accuracy: 0.3905\n",
      "Epoch 20/30\n",
      "1563/1563 [==============================] - 116s 74ms/step - loss: 1.7356 - accuracy: 0.3799\n",
      "Epoch 21/30\n",
      "1563/1563 [==============================] - 116s 74ms/step - loss: 2.0222 - accuracy: 0.2125\n",
      "Epoch 22/30\n",
      "1563/1563 [==============================] - 116s 74ms/step - loss: 1.9801 - accuracy: 0.2317\n",
      "Epoch 23/30\n",
      "1563/1563 [==============================] - 116s 74ms/step - loss: 1.9464 - accuracy: 0.2579\n",
      "Epoch 24/30\n",
      " 551/1563 [=========>....................] - ETA: 1:14 - loss: 1.9048 - accuracy: 0.2854"
     ]
    }
   ],
   "source": [
    "history = mm.fit(trainX, trainy, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
